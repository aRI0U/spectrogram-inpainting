Automatically generated by Mendeley Desktop 1.19.6
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Wavenet,
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
eprint = {1609.03499},
file = {:home/alain/Documents/papers/2016 - WaveNet A Generative Model for Raw Audio.pdf:pdf},
month = {sep},
title = {{WaveNet: A Generative Model for Raw Audio}},
url = {https://arxiv.org/abs/1609.03499},
year = {2016}
}
@misc{Jukebox,
abstract = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code.},
archivePrefix = {arXiv},
arxivId = {2005.00341},
author = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
booktitle = {arXiv},
eprint = {2005.00341},
file = {:home/alain/Documents/papers/2020 - Jukebox A generative model for music.pdf:pdf},
issn = {23318422},
month = {apr},
title = {{Jukebox: A generative model for music}},
url = {https://arxiv.org/abs/2005.00341},
year = {2020}
}
@inproceedings{Resnet,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8Ã— deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:home/alain/Documents/papers/2016 - Deep residual learning for image recognition.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
month = {dec},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
url = {http://arxiv.org/abs/1512.03385},
volume = {2016-Decem},
year = {2016}
}
@misc{GANSynth,
abstract = {Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.},
archivePrefix = {arXiv},
arxivId = {1902.08710},
author = {Engel, Jesse and Agrawal, Kumar Krishna and Chen, Shuo and Gulrajani, Ishaan and Donahue, Chris and Roberts, Adam},
eprint = {1902.08710},
file = {:home/alain/Documents/papers/2019 - GANSynth Adversarial Neural Audio Synthesis.pdf:pdf},
month = {feb},
title = {{GANSynth: Adversarial Neural Audio Synthesis}},
url = {http://arxiv.org/abs/1902.08710},
year = {2019}
}
@inproceedings{VQVAE,
abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" - where the latents are ignored when they are paired with a powerful autoregressive decoder - typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
archivePrefix = {arXiv},
arxivId = {1711.00937},
author = {van den Oord, A{\"{a}}ron and Vinyals, Oriol and Kavukcuoglu, Koray},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1711.00937},
file = {:home/alain/Documents/papers/2017 - Neural discrete representation learning.pdf:pdf},
issn = {10495258},
month = {nov},
pages = {6307--6316},
title = {{Neural discrete representation learning}},
url = {https://arxiv.org/abs/1711.00937},
volume = {2017-Decem},
year = {2017}
}
@misc{NSynth,
archivePrefix = {arXiv},
arxivId = {1704.01279},
author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
eprint = {1704.01279},
file = {:home/alain/Documents/papers/2017 - Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders.pdf:pdf},
month = {apr},
title = {{Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders}},
url = {https://arxiv.org/abs/1704.01279},
year = {2017}
}
@article{WavenetSpeech,
abstract = {We consider the task of unsupervised extraction of meaningful latent representations of speech by applying autoencoding neural networks to speech waveforms. The goal is to learn a representation able to capture high level semantic content fromthe signal, e.g. phoneme identities,while being invariant to confounding low level details in the signal such as the underlying pitch contour or background noise. Since the learned representation is tuned to contain only phonetic content, we resort to using a high capacity WaveNet decoder to infer information discarded by the encoder from previous samples. Moreover, the behavior of autoencoder models depends on the kind of constraint that is applied to the latent representation.We compare three variants: a simple dimensionality reduction bottleneck, a GaussianVariationalAutoencoder (VAE), and a discrete Vector Quantized VAE (VQ-VAE). We analyze the quality of learned representations in terms of speaker independence, the ability to predict phonetic content, and the ability to accurately reconstruct individual spectrogram frames. Moreover, for discrete encodings extracted using theVQ-VAE, we measure the ease of mapping them to phonemes.We introduce a regularization scheme that forces the representations to focus on the phonetic content of the utterance and report performance comparable with the top entries in the ZeroSpeech 2017 unsupervised acoustic unit discovery task.},
archivePrefix = {arXiv},
arxivId = {1901.08810},
author = {Chorowski, Jan and Weiss, Ron J. and Bengio, Samy and {Van Den Oord}, A{\"{a}}ron},
doi = {10.1109/TASLP.2019.2938863},
eprint = {1901.08810},
file = {:home/alain/Documents/papers/2019 - Unsupervised speech representation learning using WaveNet autoencoders.pdf:pdf},
issn = {23299304},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Acoustic unit discovery,Autoencoder,Speech representation learning,Unsupervised learning,WavenetSpeech},
mendeley-groups = {ML-project},
mendeley-tags = {WavenetSpeech},
month = {jan},
number = {12},
pages = {2041--2053},
title = {{Unsupervised speech representation learning using WaveNet autoencoders}},
url = {https://arxiv.org/abs/1901.08810},
volume = {27},
year = {2019}
}
@article{Energy,
abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
file = {:home/alain/Documents/papers/2019 - Energy and policy considerations for deep learning in NLP.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Energy and policy considerations for deep learning in NLP}},
year = {2019}
}

@misc{VQVAE2,
      title={Generating Diverse High-Fidelity Images with VQ-VAE-2}, 
      author={Ali Razavi and Aaron van den Oord and Oriol Vinyals},
      year={2019},
      eprint={1906.00446},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Carbontracker,
    abstract = {Deep learning (DL) can achieve impressive results across a wide variety of tasks, but this often comes at the cost of training models for extensive periods on specialized hardware accelerators. This energy-intensive workload has seen immense growth in recent years. Machine learning (ML) may become a significant contributor to climate change if this exponential trend continues. If practitioners are aware of their energy and carbon footprint, then they may actively take steps to reduce it whenever possible. In this work, we present Carbontracker, a tool for tracking and predicting the energy and carbon footprint of training DL models. We propose that energy and carbon footprint of model development and training is reported alongside performance metrics using tools like Carbontracker. We hope this will promote responsible computing in ML and encourage research into energy-efficient deep neural networks.},
    archivePrefix = {arXiv},
    arxivId = {2007.03051},
    author = {Anthony, Lasse F. Wolff and Kanding, Benjamin and Selvan, Raghavendra},
    eprint = {2007.03051},
    file = {:home/alain/Documents/papers/2020 - Carbontracker Tracking and Predicting the Carbon Footprint of Training Deep Learning Models.pdf:pdf},
    month = {jul},
    title = {{Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models}},
    url = {http://arxiv.org/abs/2007.03051},
    year = {2020}
}
@inproceedings{VAE,
    abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
    archivePrefix = {arXiv},
    arxivId = {1312.6114},
    author = {Kingma, Diederik P and Welling, Max},
    booktitle = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
    eprint = {1312.6114},
    file = {:home/alain/Documents/papers/2014 - Auto-encoding variational bayes.pdf:pdf},
    month = {dec},
    title = {{Auto-encoding variational bayes}},
    url = {http://arxiv.org/abs/1312.6114},
    year = {2014}
}
@article{UnderstandingVQVAE,
    author = {Yadav, Shashank},
    journal = {Noteworthy - The Journal Blog},
    title = {{Understanding Vector Quantized Variational Autoencoders (VQ-VAE)}},
    url = {https://blog.usejournal.com/understanding-vector-quantized-variational-autoencoders-vq-vae-323d710a888a},
    year = {2019}
}

@article{UnderstandingVAE,
    abstract = {Building, step by step, the reasoning that leads to VAEs.},
    author = {Rocca, Joseph and Rocca, Baptiste},
    journal = {Towards Data Science},
    mendeley-groups = {Sony/Reviews},
    title = {{Understanding Variational Autoencoders (VAEs)}},
    url = {https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73},
    year = {2019}
}

@inproceedings{Transformers,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1706.03762},
file = {:home/alain/Documents/papers/2017 - Attention is all you need.pdf:pdf},
issn = {10495258},
mendeley-groups = {Sony,Sony/Transformers},
month = {jun},
pages = {5999--6009},
title = {{Attention is all you need}},
url = {https://arxiv.org/abs/1706.03762},
volume = {2017-Decem},
year = {2017}
}

@misc{LinearTransformers,
      title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}, 
      author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and FranÃ§ois Fleuret},
      year={2020},
      eprint={2006.16236},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{MelSpectrogram,
    author = {Xu, Min and Duan, Ling-Yu and Cai, Jianfei and Chia, Liang-Tien and Xu, Changsheng and Tian, Qi},
    year = {2004},
    month = {11},
    pages = {566-574},
    title = {HMM-Based Audio Keyword Generation},
    booktitle={Advances in Multimedia Information Processing - PCM 2004},
    volume = {3333},
    isbn = {978-3-540-23985-7},
    doi = {10.1007/978-3-540-30543-9_71}
}

@misc{PhaseFeatures,
      title={Improving DNN-based Music Source Separation using Phase Features}, 
      author={Joachim Muth and Stefan Uhlich and Nathanael Perraudin and Thomas Kemp and Fabien Cardinaux and Yuki Mitsufuji},
      year={2018},
      eprint={1807.02710},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}