{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels  ,  in_channels*2, 5)\n",
    "        self.conv2 = nn.Conv2d(in_channels*2, out_channels  , 5)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size=5)\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels//2, out_channels, kernel_size=5)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_codewords, codewords_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        \n",
    "        self.num_codewords = num_codewords\n",
    "        self.codewords_dim = codewords_dim\n",
    "        self.codewords = nn.Parameter(torch.rand(self.num_codewords, self.codewords_dim),requires_grad=True)\n",
    "\n",
    "        self.commitment_cost = commitment_cost\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Reshape: B, C, H, W -> B*H*W, C\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        flat_inputs = inputs.view(-1, self.codewords_dim)\n",
    "\n",
    "        # Calculating distances:\n",
    "        distances = torch.pow(flat_inputs.unsqueeze(1) - self.codewords.unsqueeze(0),2).sum(2)\n",
    "\n",
    "        # Argmin:\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "\n",
    "        # Index from dictionary:\n",
    "        # quantized[i,j] = self.codewords[encoding_indices[i,j], j]\n",
    "        quantized = torch.gather(self.codewords,0,encoding_indices.unsqueeze(1).expand(-1, self.codewords_dim)).view(inputs.shape)\n",
    "        encoding_indices = encoding_indices.view(inputs.shape[:-1])\n",
    "\n",
    "        # quantization loss\n",
    "        quantizing_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        commitment_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = quantizing_loss + self.commitment_cost * commitment_loss\n",
    "\n",
    "        # magic trick to copy gradients from inputs\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        \n",
    "        #Reshape:\n",
    "        return quantized.permute(0, 3, 1, 2), encoding_indices, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize input (limited to the first 1000 values)\n",
    "input = F.interpolate(mnist_trainset.data.unsqueeze(1).float(), size=(32,32))\n",
    "\n",
    "#construction target\n",
    "target = torch.zeros(len(input),10)\n",
    "value = mnist_trainset.targets[:len(input)]\n",
    "for ind in range(len(target)):\n",
    "    target[ind,value[ind]] = 1\n",
    "    \n",
    "#data loader\n",
    "dataloader = torch.utils.data.DataLoader(input, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(1, 16)\n",
    "quantizer = VectorQuantizer(10, 16, 0.15)\n",
    "decoder = Decoder(16, 1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 553371270381568.0\n",
      "Epoch 1: loss = 531466400301056.0\n",
      "Epoch 2: loss = 510415322742784.0\n",
      "Epoch 3: loss = inf\n",
      "Epoch 4: loss = 1.5259345067387967e+32\n",
      "Epoch 5: loss = 1.4655176168339983e+32\n",
      "Epoch 6: loss = 1.4074827943641685e+32\n",
      "Epoch 7: loss = 1.351732938407476e+32\n",
      "Epoch 8: loss = 1.2982229802093654e+32\n",
      "Epoch 9: loss = 1.2467859912926644e+32\n",
      "Epoch 10: loss = 1.197432997060848e+32\n",
      "Epoch 11: loss = 1.149999003318055e+32\n",
      "Epoch 12: loss = 1.1044683423856632e+32\n",
      "Epoch 13: loss = 1.0607354025040712e+32\n",
      "Epoch 14: loss = 1.0187357721056099e+32\n",
      "Epoch 15: loss = 9.783769925435951e+31\n",
      "Epoch 16: loss = 9.39645330420716e+31\n",
      "Epoch 17: loss = 9.024196997268801e+31\n",
      "Epoch 18: loss = 8.667004873183496e+31\n",
      "Epoch 19: loss = 8.323811142948672e+31\n",
      "Epoch 20: loss = 7.994121114118944e+31\n",
      "Epoch 21: loss = 7.677599672457114e+31\n",
      "Epoch 22: loss = 7.373561598808624e+31\n",
      "Epoch 23: loss = 7.081513167868745e+31\n",
      "Epoch 24: loss = 6.801129903947491e+31\n",
      "Epoch 25: loss = 6.5318353912140705e+31\n",
      "Epoch 26: loss = 6.2730478945640845e+31\n",
      "Epoch 27: loss = 6.024767413997533e+31\n",
      "Epoch 28: loss = 5.78613126004954e+31\n",
      "Epoch 29: loss = 5.55700113160634e+31\n",
      "Epoch 30: loss = 5.336971796733199e+31\n",
      "Epoch 31: loss = 5.125570323649483e+31\n",
      "Epoch 32: loss = 4.922583941410942e+31\n",
      "Epoch 33: loss = 4.727679470061688e+31\n",
      "Epoch 34: loss = 4.540477306894362e+31\n",
      "Epoch 35: loss = 4.3606955304078315e+31\n",
      "Epoch 36: loss = 4.187970495715555e+31\n",
      "Epoch 37: loss = 4.0221150611006573e+31\n",
      "Epoch 38: loss = 3.8628678568009367e+31\n",
      "Epoch 39: loss = 3.7098717661292794e+31\n",
      "Epoch 40: loss = 3.563021612539379e+31\n",
      "Epoch 41: loss = 3.4219097462448615e+31\n",
      "Epoch 42: loss = 3.286410197175323e+31\n",
      "Epoch 43: loss = 3.156250473451022e+31\n",
      "Epoch 44: loss = 3.031235696229837e+31\n",
      "Epoch 45: loss = 2.911205803733251e+31\n",
      "Epoch 46: loss = 2.7959398043214387e+31\n",
      "Epoch 47: loss = 2.6851850324981e+31\n",
      "Epoch 48: loss = 2.578876931624468e+31\n",
      "Epoch 49: loss = 2.476770089759161e+31\n",
      "Epoch 50: loss = 2.378677365185302e+31\n",
      "Epoch 51: loss = 2.2844928560010933e+31\n",
      "Epoch 52: loss = 2.194012495528184e+31\n",
      "Epoch 53: loss = 2.107124820806005e+31\n",
      "Epoch 54: loss = 2.0236681984524747e+31\n",
      "Epoch 55: loss = 1.9435638065041537e+31\n",
      "Epoch 56: loss = 1.8665727612190862e+31\n",
      "Epoch 57: loss = 1.7926627842778886e+31\n",
      "Epoch 58: loss = 1.7216929149299938e+31\n",
      "Epoch 59: loss = 1.6535143344070071e+31\n",
      "Epoch 60: loss = 1.588028757039794e+31\n",
      "Epoch 61: loss = 1.5251414030440967e+31\n",
      "Epoch 62: loss = 1.4647327096563553e+31\n",
      "Epoch 63: loss = 1.4067508139589082e+31\n",
      "Epoch 64: loss = 1.3510419405875006e+31\n",
      "Epoch 65: loss = 1.2975242452641445e+31\n",
      "Epoch 66: loss = 1.2461490082783095e+31\n",
      "Epoch 67: loss = 1.196798722040329e+31\n",
      "Epoch 68: loss = 1.149416567036681e+31\n",
      "Epoch 69: loss = 1.10388261782606e+31\n",
      "Epoch 70: loss = 1.0601726958920734e+31\n",
      "Epoch 71: loss = 1.0182019346421845e+31\n",
      "Epoch 72: loss = 9.778638881579761e+30\n",
      "Epoch 73: loss = 9.39159221348649e+30\n",
      "Epoch 74: loss = 9.019622059289634e+30\n",
      "Epoch 75: loss = 8.662474544567072e+30\n",
      "Epoch 76: loss = 8.319431567381954e+30\n",
      "Epoch 77: loss = 7.989852397049885e+30\n",
      "Epoch 78: loss = 7.67352547155243e+30\n",
      "Epoch 79: loss = 7.369626907943525e+30\n",
      "Epoch 80: loss = 7.077878048821746e+30\n",
      "Epoch 81: loss = 6.797529360178933e+30\n",
      "Epoch 82: loss = 6.528322131889688e+30\n",
      "Epoch 83: loss = 6.26979455429092e+30\n",
      "Epoch 84: loss = 6.021548890787973e+30\n",
      "Epoch 85: loss = 5.783053214020219e+30\n",
      "Epoch 86: loss = 5.554034911215333e+30\n",
      "Epoch 87: loss = 5.334211093731527e+30\n",
      "Epoch 88: loss = 5.122909598813094e+30\n",
      "Epoch 89: loss = 4.920113199267104e+30\n",
      "Epoch 90: loss = 4.7252168277208416e+30\n",
      "Epoch 91: loss = 4.5380488167079204e+30\n",
      "Epoch 92: loss = 4.3584090890051944e+30\n",
      "Epoch 93: loss = 4.185785060065147e+30\n",
      "Epoch 94: loss = 4.0199932753946516e+30\n",
      "Epoch 95: loss = 3.860827310910009e+30\n",
      "Epoch 96: loss = 3.7079417160582647e+30\n",
      "Epoch 97: loss = 3.561139738162276e+30\n",
      "Epoch 98: loss = 3.420113403369496e+30\n",
      "Epoch 99: loss = 3.284655985364771e+30\n",
      "Epoch 100: loss = 3.154581007340425e+30\n",
      "Epoch 101: loss = 3.029669955954563e+30\n",
      "Epoch 102: loss = 2.909675303645619e+30\n",
      "Epoch 103: loss = 2.794473437748537e+30\n",
      "Epoch 104: loss = 2.6838258976453984e+30\n",
      "Epoch 105: loss = 2.577541068593794e+30\n",
      "Epoch 106: loss = 2.475445771970064e+30\n",
      "Epoch 107: loss = 2.377453871809561e+30\n",
      "Epoch 108: loss = 2.2832605676900146e+30\n",
      "Epoch 109: loss = 2.1928569437835052e+30\n",
      "Epoch 110: loss = 2.106042771751159e+30\n",
      "Epoch 111: loss = 2.0226368638357615e+30\n",
      "Epoch 112: loss = 1.9425228609271745e+30\n",
      "Epoch 113: loss = 1.865588030692719e+30\n",
      "Epoch 114: loss = 1.791750317292389e+30\n",
      "Epoch 115: loss = 1.72077926924182e+30\n",
      "Epoch 116: loss = 1.6526464767842513e+30\n",
      "Epoch 117: loss = 1.5871937217530407e+30\n",
      "Epoch 118: loss = 1.52435330430229e+30\n",
      "Epoch 119: loss = 1.4639679129597216e+30\n",
      "Epoch 120: loss = 1.405993875280102e+30\n",
      "Epoch 121: loss = 1.3503376506281386e+30\n",
      "Epoch 122: loss = 1.2968390563327325e+30\n",
      "Epoch 123: loss = 1.2455065548746208e+30\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "for x in dataloader:\n",
    "    \n",
    "    z_e = encoder(x)\n",
    "    z_q, codes, q_loss = quantizer(z_e)\n",
    "    x_hat = decoder(z_q)\n",
    "    \n",
    "    #calcul loss\n",
    "    rec_loss = criterion(x_hat, x)\n",
    "    loss = rec_loss + q_loss\n",
    "    print('Epoch {}: loss = {}'.format(epoch,loss.item()))\n",
    "    \n",
    "    #backprop\n",
    "    decoder.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    for f in encoder.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "        \n",
    "    for f in decoder.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "        \n",
    "    for f in quantizer.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "    \n",
    "    epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = F.interpolate(mnist_testset.data.unsqueeze(1).float(), size=(32,32))\n",
    "output = net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = torch.argmax(output,axis=1) == mnist_testset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.8"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(comp).item()/len(comp)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 2, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 16]' is invalid for input of size 24",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-a4bd5592dd72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 16]' is invalid for input of size 24"
     ]
    }
   ],
   "source": [
    "torch.zeros(2,3,4).permute(0,2,1).view(-1,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
