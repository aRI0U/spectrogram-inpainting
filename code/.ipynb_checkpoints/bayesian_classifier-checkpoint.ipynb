{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Bayesian Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import skimage.transform as transform\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate codemap example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a7d3847f98>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Codemap size : 32 * 4\n",
    "size_cm = [32,8]\n",
    "limit_cm = [0,128]\n",
    "test_codemap = torch.randint(limit_cm[0], limit_cm[1], (size_cm[0], size_cm[1]))\n",
    "test_cm_np = test_codemap.numpy().astype(np.uint8)\n",
    "\n",
    "plt.imshow(test_codemap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample one element of a codemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel's coordonates : [1 3]\n",
      "Value of the pixel : 0\n"
     ]
    }
   ],
   "source": [
    "# Init variables\n",
    "coord_pixel = []\n",
    "\n",
    "# Mouse function to select the pixel to resample\n",
    "def select_pixel(event,x,y,flags,param):\n",
    "    global coord_pixel\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        coord_pixel = [x, y]\n",
    "\n",
    "# Resample element of codemap\n",
    "cv2.namedWindow('image')\n",
    "cv2.setMouseCallback('image', select_pixel)\n",
    "width = 160\n",
    "ratio = width / np.shape(test_cm_np)[1]\n",
    "test_cm_resized =  imutils.resize(test_cm_np, width = width)\n",
    "while(True):\n",
    "    cv2.imshow('image', test_cm_resized)\n",
    "    # Select pixel to resample\n",
    "    if cv2.waitKey(0) == 13: # press Enter to exit after selection\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Display\n",
    "coord_pixel = (np.floor(np.divide(coord_pixel, ratio))).astype(int)\n",
    "pixel_x = coord_pixel[0]\n",
    "pixel_y = coord_pixel[1]\n",
    "print(\"Pixel's coordonates : \" + str(coord_pixel))\n",
    "print(\"Value of the pixel : \" + str(test_cm_np[pixel_y][pixel_x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel's coordonates : [1 3]\n",
      "Value predicted : 43\n"
     ]
    }
   ],
   "source": [
    "# Get the surroundings of the pixel\n",
    "nb_ngbrs = 2 # number of neighbours to add to the bayesian distribution\n",
    "ind_max_fr = min(pixel_y + nb_ngbrs + 1, size_cm[0])\n",
    "ind_min_fr = max(pixel_y - nb_ngbrs, 0)\n",
    "ind_max_tm = min(pixel_x + nb_ngbrs + 1, size_cm[1])\n",
    "ind_min_tm = max(pixel_x - nb_ngbrs, 0)\n",
    "coord_ngbrs = []\n",
    "values_ngbrs = []\n",
    "\n",
    "for i in range(ind_min_tm, ind_max_tm):\n",
    "    for j in range(ind_min_fr, ind_max_fr):\n",
    "        if abs(i-pixel_x) + abs(j-pixel_y) <= nb_ngbrs and (i!=pixel_x or j!=pixel_y):\n",
    "            coord_ngbrs.append([i, j])\n",
    "            values_ngbrs.append(test_cm_np[j][i])\n",
    "\n",
    "# Init classifier\n",
    "clf = GaussianNB()\n",
    "# Fit distribution to the neighbors of pixel\n",
    "clf.fit(coord_ngbrs, values_ngbrs)\n",
    "# Predict value\n",
    "proba_predicted = clf.predict_proba(np.array(coord_pixel).reshape(1, -1))\n",
    "classes = np.unique(values_ngbrs)\n",
    "value_predicted = int(round(np.sum(proba_predicted * classes)))\n",
    "\n",
    "print(\"Pixel's coordonates : \" + str(coord_pixel))\n",
    "print(\"Value predicted : \" + str(value_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a7d3bdbdd8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Resample codemap according to value predicted\n",
    "codemap_resampled = test_codemap\n",
    "codemap_resampled[pixel_y][pixel_x] = value_predicted\n",
    "\n",
    "plt.imshow(codemap_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Sampling to MNIST codemaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init MNIST parameters\n",
    "batch_size_test = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test samples\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                            transform=torchvision.transforms.Compose([\n",
    "                            torchvision.transforms.ToTensor(),\n",
    "                            torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                            ])),\n",
    "                    batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on some examples\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "np.shape(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display\n",
    "for i in range(0,3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import MNIST Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels*2, 5)\n",
    "        self.conv2 = nn.Conv2d(in_channels*2, out_channels, 5)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_codewords, codewords_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        \n",
    "        self.num_codewords = num_codewords\n",
    "        self.codewords_dim = codewords_dim\n",
    "        self.codewords = nn.Parameter(torch.rand(self.num_codewords, self.codewords_dim),requires_grad=True)\n",
    "\n",
    "        self.commitment_cost = commitment_cost\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Reshape: B, C, H, W -> B*H*W, C\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        flat_inputs = inputs.view(-1, self.codewords_dim)\n",
    "\n",
    "        # Calculating distances:\n",
    "        distances = torch.pow(flat_inputs.unsqueeze(1) - self.codewords.unsqueeze(0),2).sum(2)\n",
    "\n",
    "        # Argmin:\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "\n",
    "        # Index from dictionary:\n",
    "        # quantized[i,j] = self.codewords[encoding_indices[i,j], j]\n",
    "        quantized = torch.gather(self.codewords,0,encoding_indices.unsqueeze(1).expand(-1, self.codewords_dim)).view(inputs.shape)\n",
    "        encoding_indices = encoding_indices.view(inputs.shape[:-1])\n",
    "\n",
    "        # quantization loss\n",
    "        quantizing_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        commitment_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = quantizing_loss + self.commitment_cost * commitment_loss\n",
    "\n",
    "        # magic trick to copy gradients from inputs\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        \n",
    "        #Reshape:\n",
    "        return quantized.permute(0, 3, 1, 2), encoding_indices, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size=5)\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels//2, out_channels, kernel_size=5)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self,x_dim,z_dim,num_codewords,commitment_cost):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(x_dim,z_dim)\n",
    "        self.quantizer = VectorQuantizer(num_codewords, z_dim, commitment_cost)\n",
    "        self.decoder = Decoder(z_dim, x_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_e = self.encoder(x)\n",
    "        z_q, codes, q_loss = self.quantizer(z_e)\n",
    "        x_hat = self.decoder(z_q)\n",
    "        \n",
    "        return x_hat, codes, q_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/results/MNIST/model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-9c2d802f9e67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mMNIST_state_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/results/MNIST/model.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mMNIST_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVQVAE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mMNIST_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMNIST_state_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/results/MNIST/model.pth'"
     ]
    }
   ],
   "source": [
    "MNIST_state_dict = torch.load('/results/MNIST/model.pth')\n",
    "MNIST_model = VQVAE(1,16,10,0.15).to(device)\n",
    "MNIST_model.load_state_dict(MNIST_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple operations on MNIST Codemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MNIST_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a1bca100b3ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcodemaps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMNIST_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodemaps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MNIST_model' is not defined"
     ]
    }
   ],
   "source": [
    "codemaps = MNIST_model.encoder(example_data).detach()\n",
    "np.shape(codemaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codemap_mix = codemaps[0]\n",
    "for i in range(8,16):\n",
    "    codemap_mix[i] = codemaps[1][i]\n",
    "codemaps[0] = codemap_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display\n",
    "for i in range(0,16):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.imshow(codemap_mix[i], cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_q, codes, q_loss = MNIST_model.quantizer(codemaps)\n",
    "output = MNIST_model.decoder(z_q).detach()\n",
    "np.shape(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output[0][0], cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling of MNIST codemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to sample\n",
    "example_data_np = example_data.detach().numpy()\n",
    "min_MNIST = abs(np.min(example_data_np))\n",
    "max_MNIST = np.max(example_data_np + min_MNIST)\n",
    "example_data0 = (example_data_np[0][0])\n",
    "\n",
    "# Transform example to discrete values\n",
    "example0 = example_data0 + min_MNIST\n",
    "example0 = 255 * np.divide(example0,max_MNIST)  # Scale by 255\n",
    "example0 = example0.astype(np.uint8)\n",
    "\n",
    "plt.imshow(example0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt image to opencv display\n",
    "width = 640\n",
    "ratio = width / np.shape(example0)[1]\n",
    "example0_resized =  imutils.resize(example0, width = width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select ROI\n",
    "fromCenter = False\n",
    "selection_box = cv2.selectROI(cv2.cvtColor(example0_resized,cv2.COLOR_GRAY2RGB), fromCenter) # press Enter to exit after selection\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Resized box to original size\n",
    "sample_box = (np.round(np.divide(selection_box, ratio))).astype(int)\n",
    "\n",
    "# Show cropped image according to sample_box\n",
    "plt.imshow(example0[sample_box[1]:sample_box[1]+sample_box[3], sample_box[0]:sample_box[0]+sample_box[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to do inpainting in the defined area\n",
    "def bayesian_inpainting(im, sample_box, nb_ngbrs, autoregressive):\n",
    "    # Init sampled image\n",
    "    im_sampled = im.copy()\n",
    "    size_im = np.shape(im)\n",
    "\n",
    "    # Get every element in the sample box\n",
    "    X_coord, Y_coord = np.mgrid[sample_box[0]:sample_box[0]+sample_box[2], sample_box[1]:sample_box[1]+sample_box[3]]\n",
    "    XY_coord = np.vstack((X_coord.ravel(), Y_coord.ravel()))\n",
    "    \n",
    "    for x, y in zip(XY_coord[0], XY_coord[1]):\n",
    "        # Get the surroundings of the pixel\n",
    "        ind_max_fr = min(y + nb_ngbrs + 1, size_im[0])\n",
    "        ind_min_fr = max(y - nb_ngbrs, 0)\n",
    "        ind_max_tm = min(x + nb_ngbrs + 1, size_im[1])\n",
    "        ind_min_tm = max(x - nb_ngbrs, 0)\n",
    "        coord_ngbrs = []\n",
    "        values_ngbrs = []\n",
    "        for i in range(ind_min_tm, ind_max_tm):\n",
    "            for j in range(ind_min_fr, ind_max_fr):\n",
    "                if abs(i-x) + abs(j-y) <= nb_ngbrs and (i!=x or j!=y):\n",
    "                    coord_ngbrs.append([i, j])\n",
    "                    if autoregressive:\n",
    "                        values_ngbrs.append(im_sampled[j][i])\n",
    "                    else:\n",
    "                        values_ngbrs.append(im[j][i])\n",
    "        # Init classifier\n",
    "        clf = GaussianNB()\n",
    "        # Fit distribution to the neighbors of pixel\n",
    "        clf.fit(coord_ngbrs, values_ngbrs)\n",
    "        # Predict value\n",
    "        proba_predicted = clf.predict_proba(np.array(coord_pixel).reshape(1, -1))\n",
    "        classes = np.unique(values_ngbrs)\n",
    "        value_predicted = int(round(np.sum(proba_predicted * classes)))\n",
    "\n",
    "        # Sample the element selected\n",
    "        im_sampled[y][x] = value_predicted\n",
    "\n",
    "    return im_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_ngbrs = 2 # number of neighbours to add to the bayesian distribution\n",
    "autoregressive = True # choose if the sampling method is autoregressive or not\n",
    "example0_resampled = bayesian_inpainting(example0, sample_box, nb_ngbrs, autoregressive)\n",
    "\n",
    "# Display\n",
    "plt.imshow(example0_resampled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
