{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels  ,  in_channels*2, 5)\n",
    "        self.conv2 = nn.Conv2d(in_channels*2, out_channels  , 5)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size=5)\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels//2, out_channels, kernel_size=5)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_codewords, codewords_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        \n",
    "        self.num_codewords = num_codewords\n",
    "        self.codewords_dim = codewords_dim\n",
    "        self.codewords = nn.Parameter(torch.rand(self.num_codewords, self.codewords_dim),requires_grad=True)\n",
    "\n",
    "        self.commitment_cost = commitment_cost\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Reshape: B, C, H, W -> B*H*W, C\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        inputs_shape = inputs.size()\n",
    "        flat_inputs = inputs.view(-1, self.codewords_dim)\n",
    "\n",
    "        # Calculating distances:\n",
    "        distances = torch.pow(flat_inputs.unsqueeze(1) - self.codewords.unsqueeze(0),2).sum(2)\n",
    "        \n",
    "        # Argmin:\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "\n",
    "        # quantized[i,j] = self.codewords[encoding_indices[i,j], j]\n",
    "        quantized = torch.gather(self.codewords,0,encoding_indices.unsqueeze(1).expand(-1, self.codewords_dim)).view(inputs_shape)\n",
    "        encoding_indices = encoding_indices.view(inputs_shape[:-1])\n",
    "\n",
    "        # quantization loss\n",
    "        quantizing_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        commitment_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = quantizing_loss + self.commitment_cost * commitment_loss\n",
    "\n",
    "        # magic trick to copy gradients from inputs\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        \n",
    "        #Reshape:\n",
    "        return quantized.permute(0, 3, 1, 2), encoding_indices, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize input (limited to the first 1000 values)\n",
    "input = F.interpolate(mnist_trainset.data.unsqueeze(1).float(), size=(32,32))[:1000]\n",
    "\n",
    "#construction target\n",
    "target = torch.zeros(len(input),10)\n",
    "value = mnist_trainset.targets[:len(input)]\n",
    "for ind in range(len(target)):\n",
    "    target[ind,value[ind]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(1, 16)\n",
    "decoder = Decoder(16, 1)\n",
    "quantizer = VectorQuantizer(10, 16, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 6655.23486328125\n",
      "1: 6655.23486328125\n",
      "2: 6655.23486328125\n",
      "3: 6655.23486328125\n",
      "4: 6655.23486328125\n",
      "5: 6655.23486328125\n",
      "6: 6655.23486328125\n",
      "7: 6655.23486328125\n",
      "8: 6655.23486328125\n",
      "9: 6655.23486328125\n"
     ]
    }
   ],
   "source": [
    "for ind in range(5):\n",
    "    #forward\n",
    "    x = input\n",
    "    \n",
    "    z_e = encoder(x)\n",
    "    z_q, codes, q_loss = quantizer(z_e)\n",
    "    x_hat = decoder(z_q)\n",
    "    \n",
    "    #calcul loss\n",
    "    rec_loss = F.mse_loss(x_hat, x)\n",
    "    loss = rec_loss + q_loss\n",
    "    print('{}: {}'.format(ind,loss.item()))\n",
    "    \n",
    "    #backprop\n",
    "    decoder.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    learning_rate = 0.02\n",
    "    \n",
    "    for f in encoder.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = F.interpolate(mnist_testset.data.unsqueeze(1).float(), size=(32,32))\n",
    "output = net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = torch.argmax(output,axis=1) == mnist_testset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.8"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(comp).item()/len(comp)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 2, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
